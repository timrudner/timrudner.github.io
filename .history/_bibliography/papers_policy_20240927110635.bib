---
---

@inproceedings{
    probasco2024dss,
    title = {Not Oracles of the Battlefield: Safety Considerations for AI-Based Military Decision Support Systems},
    author = {Emilia Probasco and Matthew Burtell and Helen Toner and Tim G. J. Rudner},
    booktitle = {AAAI Conference on Artificial Intelligence, Ethics, and Society (Forthcoming)},
    booktitle_show = {AAAI Conference on Artificial Intelligence, Ethics, and Society (Forthcoming)},
    booktitle_abbr = {AIES},
    year = {2024},
    bibtex_show = {true},
    abstract = {AI-based military decision support systems that help commanders observe, orient, decide, and act on the battlefield are highly sought after by military leadership. With the advent of large language models, AI developers have begun advertising automated AI-based decision support systems designed to both analyze and act on data from the battlefield. While the desire to use decision support systems to make better decisions on the battlefield is unsurprising, the responsible deployment of such systems requires a clear understanding of the capabilities and limitations of modern machine learning models. This paper reviews recently proposed uses of AI-enables decision support systems (DSS), provides a simplified framework for considering AI-DSS capabilities and limitations, and recommends practical risk mitigations commanders might employ when operating with an AI-enabled DSS.},
    selected = {true},
}

@inproceedings{
    narayanan2024interpretability,
    author = {Mina Narayanan and Christian Schoeberl and Tim G. J. Rudner},
    title = {{E}valuating {E}xplainability {C}laims {i}s {N}ot {S}elf-{E}xplanatory},
    booktitle = {CSET Issue Briefs (Forthcoming)},
    booktitle_show = {CSET Issue Briefs (Forthcoming)},
    year = {2024},
    bibtex_show = {true},
    abstract = {},
    selected = {true},
    placeholder = {},
}

@inproceedings{
    rudner2024uncertainty,
    author = {Rudner, Tim G. J. and Toner, Helen},
    title = {{K}ey {C}oncepts in {AI} {S}afety: {R}eliable {U}ncertainty {Q}uantification in {M}achine {L}earning},
    booktitle = {CSET Issue Briefs},
    booktitle_show = {CSET Issue Briefs},
    year = {2024},
    bibtex_show = {true},
    abstract = {},
    url = {https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-reliable-uncertainty-quantification-in-machine-learning},
    pdf = {https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-reliable-uncertainty-quantification-in-machine-learning},
    selected = {true},
    placeholder = {},
}

@inproceedings{
    oecd2022classification,
    author = {OECD {(as a contributing author)}},
    title = {OECD Framework for the Classification of AI systems},
    booktitle = {{OECD} {D}igital {E}conomy {P}apers},
    booktitle_show = {{OECD} {D}igital {E}conomy {P}apers},
    year = {2022},
    bibtex_show = {true},
    abstract = {As artificial intelligence (AI) integrates all sectors at a rapid pace, different AI systems bring different benefits and risks. In comparing virtual assistants, self-driving vehicles and video recommendations for children, it is easy to see that the benefits and risks of each are very different. Their specificities will require different approaches to policy making and governance. To help policy makers, regulators, legislators and others characterise AI systems deployed in specific contexts, the OECD has developed a user-friendly tool to evaluate AI systems from a policy perspective. It can be applied to the widest range of AI systems across the following dimensions: People & Planet; Economic Context; Data & Input; AI Model; and Task & Output. Each of the framework's dimensions has a subset of properties and attributes to define and assess policy implications and to guide an innovative and trustworthy approach to AI as outlined in the OECD AI Principles.},
    number = {323}, 
    url = {https://www.oecd-ilibrary.org/content/paper/cb6d9eca-en},
    pdf = {https://www.oecd-ilibrary.org/content/paper/cb6d9eca-en},
    doi = {https://doi.org/https://doi.org/10.1787/cb6d9eca-en},
    selected = {true},
    placeholder = {},
}

@inproceedings{
    rudner2021specification,
    author = {Rudner, Tim G. J. and Toner, Helen},
    title = {{K}ey {C}oncepts in {AI} {S}afety: {S}pecification in {M}achine {L}earning},
    booktitle = {CSET Issue Briefs},
    booktitle_show = {CSET Issue Briefs},
    year = {2021},
    bibtex_show = {true},
    abstract = {This paper is the fourth installment in a series on "AI safety," an area of machine learning research that aims to identify causes of unintended behavior in machine learning systems and develop tools to ensure these systems work safely and reliably. The first paper in the series, “Key Concepts in AI Safety: An Overview,” outlined three categories of AI safety issues—problems of robustness, assurance, and specification—and the subsequent two papers described problems of robustness and assurance, respectively. This paper introduces specification as a key element in designing modern machine learning systems that operate as intended.},
    url = {https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-specification-in-machine-learning/},
    pdf = {https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-specification-in-machine-learning/},
    selected = {true},
    preview = {rudner2021specification.png},
    placeholder = {},
}

@inproceedings{
    rudner2021interpretability,
    author = {Rudner, Tim G. J. and Toner, Helen},
    title = {{K}ey {C}oncepts in {AI} {S}afety: {I}nterpretability in {M}achine {L}earning},
    booktitle = {CSET Issue Briefs},
    booktitle_show = {CSET Issue Briefs},
    year = {2021},
    bibtex_show = {true},
    abstract = {This paper is the third installment in a series on "AI safety," an area of machine learning research that aims to identify causes of unintended behavior in machine learning systems and develop tools to ensure these systems work safely and reliably. The first paper in the series, “Key Concepts in AI Safety: An Overview,” described three categories of AI safety issues: problems of robustness, assurance, and specification. This paper introduces interpretability as a means to enable assurance in modern machine learning systems.},
    url = {https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-interpretability-in-machine-learning},
    pdf = {https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-interpretability-in-machine-learning},
    selected = {true},
    preview = {rudner2021interpretability.png},
    placeholder = {},
}

@inproceedings{
    rudner2021robustness,
    author = {Rudner, Tim G. J. and Toner, Helen},
    title = {{K}ey {C}oncepts in {AI} {S}afety: {R}obustness and {A}dversarial {E}xamples},
    booktitle = {CSET Issue Briefs},
    booktitle_show = {CSET Issue Briefs},
    year = {2021},
    bibtex_show = {true},
    abstract = {This paper is the second installment in a series on "AI safety," an area of machine learning research that aims to identify causes of unintended behavior in machine learning systems and develop tools to ensure these systems work safely and reliably. The first paper in the series, “Key Concepts in AI Safety: An Overview,” described three categories of AI safety issues: problems of robustness, assurance, and specification. This paper introduces adversarial examples, a major challenge to robustness in modern machine learning systems.},
    url = {https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-robustness-and-adversarial-examples},
    pdf = {https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-robustness-and-adversarial-examples},
    selected = {true},
    preview = {rudner2021robustness.png},
    placeholder = {},
}

@inproceedings{
    rudner2021aisafety,
    author = {Rudner, Tim G. J. and Toner, Helen},
    title = {{K}ey {C}oncepts in {AI} {S}afety: {A}n {O}verview},
    booktitle = {CSET Issue Briefs},
    booktitle_show = {CSET Issue Briefs},
    year = {2021},
    bibtex_show = {true},
    abstract = {This paper is the first installment in a series on "AI safety," an area of machine learning research that aims to identify causes of unintended behavior in machine learning systems and develop tools to ensure these systems work safely and reliably. In it, the authors introduce three categories of AI safety issues: problems of robustness, assurance, and specification. Other papers in this series elaborate on these and further key concepts.},
    url = {https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-an-overview},
    pdf = {https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-an-overview},
    selected = {true},
    preview = {rudner2021aisafety.png},
    placeholder = {},
}
