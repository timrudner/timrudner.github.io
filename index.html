<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Tim G. J. Rudner</title> <meta name="author" content="Tim G. J. Rudner"> <meta name="description" content="I'm a machine learning researcher at New York University. My research is focused on probabilistic machine learning and AI safety. "> <meta name="keywords" content="Tim Rudner, tim rudner, Tim GJ Rudner, tim gj rudner, Tim G J Rudner, tim g j rudner, Tim G. J. Rudner, tim g. j. rudner, Tim, Georg, Johann, Rudner, G., J., Tim Georg Johann Rudner, research, machine learning, Machine Learning, probabilistic machine learning, statistics, AIMS, Rhodes scholarship, Oxford, Yale, NYU, New York University, ai safety, ml safety, reliability, uncertainty quantification, function-space"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/nyu_favicon.png"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://timrudner.com/"> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> </ul> </div> <div class="navbar-brand social">    <a href="https://twitter.com/timrudner" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Tim</span> G. J. Rudner </h1> <p class="desc"><b>Data Science Assistant Professor, Faculty Fellow, and Instructor</b>, <a href="https://cds.nyu.edu/" rel="external nofollow noopener" target="_blank">Center for Data Science</a>, New York University<br> <b>AI Fellow</b>, <a href="https://cset.georgetown.edu/" rel="external nofollow noopener" target="_blank">Center for Security &amp; Emerging Technology</a>, Georgetown University<br> </p> </header> <hr class="solid"> <article> <div class="profile float-right">  <br> <figure> <picture> <img src="/assets/img/timrudner.png?956af7c4b4e49bda2a601b723f580d9c" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="timrudner.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="address"> <p style="text-align: center;">tim.rudner [at] nyu.edu</p> <br> </div> </div> <div class="clearfix"> <p>The goal of my research is to develop methods and theoretical insights that enable the <strong>safe deployment of machine learning systems</strong> in safety-critical settings by drawing on tools from <strong>probabilistic machine learning</strong>. <nobr>I am</nobr> particularly interested in reliable uncertainty quantification in deep learning [<a href="https://openreview.net/pdf?id=9EndFTDiqh" rel="external nofollow noopener" target="_blank">1</a>,<a href="https://timrudner.com/fseb/Rudner2023_Function-Space_Regularization_in_Neural_Networks-_A_Probabilistic_Perspective.pdf">2</a>,<a href="https://openreview.net/pdf?id=OQs0pLKGGpS" rel="external nofollow noopener" target="_blank">3</a>], robustness to distribution shifts [<a href="https://arxiv.org/pdf/2207.07411.pdf" rel="external nofollow noopener" target="_blank">1</a>,<a href="https://arxiv.org/pdf/2211.12717.pdf" rel="external nofollow noopener" target="_blank">2</a>], AI-assisted clinical decision-making and drug discovery [<a href="https://proceedings.mlr.press/v202/klarner23a/klarner23a.pdf" rel="external nofollow noopener" target="_blank">1</a>,<a href="https://openreview.net/pdf?id=MfiK69Ga6p" rel="external nofollow noopener" target="_blank">2</a>], sequential decision-making [<a href="https://arxiv.org/pdf/2104.10190.pdf" rel="external nofollow noopener" target="_blank">1</a>,<a href="https://proceedings.mlr.press/v162/rudner22a/rudner22a.pdf" rel="external nofollow noopener" target="_blank">2</a>], and AI safety.</p> <p><strong>Bio:</strong> I am a Data Science Assistant Professor, Faculty Fellow, and Instructor at New York University. Before joining New York University, I conducted PhD research on probabilistic machine learning in the Department of Computer Science at the University of Oxford, where I was advised by <a href="https://www.cs.ox.ac.uk/people/yarin.gal" rel="external nofollow noopener" target="_blank"><nobr>Yarin Gal</nobr></a> and <a href="http://csml.stats.ox.ac.uk/people/teh" rel="external nofollow noopener" target="_blank">Yee Whye Teh</a>. For my work on safe decision-making under uncertainty, I received the 2021 <a href="https://www.qualcomm.com/news/releases/2021/07/26/qualcomm-innovation-fellowship-europe-rewards-excellent-young-researchers" rel="external nofollow noopener" target="_blank">Qualcomm Innovation Fellowship</a>. I care deeply about equitable access to education and was an <a href="https://www.mpls.ox.ac.uk/equality-and-diversity/mpls-ed-i-fellows" rel="external nofollow noopener" target="_blank">Equality, Diversity &amp; Inclusion Fellow</a> at the University of Oxford.</p> <p>I am also an <a href="https://cset.georgetown.edu" rel="external nofollow noopener" target="_blank">AI Fellow</a> at Georgetown’s Center for Security &amp; Emerging Technology and a <a href="https://www.rhodeshouse.ox.ac.uk/" rel="external nofollow noopener" target="_blank">Rhodes Scholar</a>.</p> <p><strong>Mentoring:</strong> I was the first in my family to attend college, and I know that navigating higher education can be challenging for first-generation low-income students. If you identify as a first-generation low-income student and are looking for mentorship, please feel free to get in touch using <a href="https://forms.gle/DbHU2m5Ws7iVEjKi9" rel="external nofollow noopener" target="_blank">this form</a>.</p> <p><strong>I am on the academic job market this year (2023-24).</strong> Please feel free to reach out if you think I would be a good fit for your department. You can find my CV <a href="https://timrudner.com/cv">here</a>.</p> </div> <br> <h2>News</h2> <div class="news"> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Sep 2023</th> <td> Four papers published at NeurIPS 2023 (including a spotlight): [<a href="https://openreview.net/pdf?id=9EndFTDiqh" rel="external nofollow noopener" target="_blank">1</a>,<a href="https://openreview.net/pdf?id=ECvtxmVP0x" rel="external nofollow noopener" target="_blank">2</a>,<a href="https://openreview.net/pdf?id=MfiK69Ga6p" rel="external nofollow noopener" target="_blank">3</a>,<a href="https://openreview.net/pdf?id=KipjqOPaZ0" rel="external nofollow noopener" target="_blank">4</a>]! </td> </tr> <tr> <th scope="row">Jul 2023</th> <td> Two papers published at ICML 2023 [<a href="https://timrudner.com/fseb">1</a>,<a href="https://proceedings.mlr.press/v202/klarner23a/klarner23a.pdf" rel="external nofollow noopener" target="_blank">2</a>] and four papers published at AABI 2023! </td> </tr> </table> </div> </div> <br> <h2>Selected Publications</h2> <div class="publications"> For a complete list, please see: [<a href="publications" target="_blank">Publications</a>] or [<a href="https://scholar.google.de/citations?hl=en&amp;user=MbBntPgAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" target="_blank" rel="external nofollow noopener">Google Scholar</a>] <br><br><br> <smaller_margin><h4>Reliable Uncertainty Quantification</h4></smaller_margin> <h2 class="bibliography"></h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="rudner2023fsmap" class="col-sm-8"> <div class="title">Should We Learn Most Likely Functions or Parameters?</div> <div class="author"> Shikai Qiu*, <em><b>Tim G. J. Rudner*</b></em>, Sanyam Kapoor, and Andrew Gordon Wilson</div> <div class="periodical"> <em>Advances in Neural Information Processing Systems</em> <b>(NeurIPS)</b>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/pdf?id=9EndFTDiqh" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://neurips.cc/virtual/2023/poster/72607" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Talk</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Standard regularized training procedures correspond to maximizing a posterior distribution over parameters, known as maximum a posteriori (MAP) estimation. However, model parameters are of interest only insomuch as they combine with the functional form of a model to provide a function that can make good predictions. Moreover, the most likely parameters under the parameter posterior do not generally correspond to the most likely function induced by the parameter posterior. In fact, we can re-parametrize a model such that any setting of parameters can maximize the parameter posterior. As an alternative, we investigate the benefits and drawbacks of directly estimating the most likely function implied by the model and the data. We show that this procedure leads to pathological solutions when using neural networks and prove conditions under which the procedure is well-behaved, as well as a scalable approximation. Under these conditions, we find that function-space MAP estimation can lead to flatter minima, better generalization, and improved robustness to overfitting.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">rudner2023fsmap</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Should We Learn Most Likely Functions or Parameters?}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Qiu*, Shikai and Rudner*, Tim G. J. and Kapoor, Sanyam and Wilson, Andrew Gordon}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems 36}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/rudner2023fseb.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="rudner2023fseb.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="rudner2023fseb" class="col-sm-8"> <div class="title">Function-Space Regularization in Neural Networks: A Probabilistic Perspective</div> <div class="author"> <em><b>Tim G. J. Rudner</b></em>, Sanyam Kapoor, Shikai Qiu, and Andrew Gordon Wilson</div> <div class="periodical"> <em>International Conference on Machine Learning</em> <b>(ICML)</b>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://timrudner.com/fseb" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/poster/rudner2023fseb.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://github.com/timrudner/function-space-empirical-bayes" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://icml.cc/virtual/2023/spotlight/24608" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Talk</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Parameter-space regularization in neural network optimization is a fundamental tool for improving generalization. However, standard parameter-space regularization methods make it challenging to encode explicit preferences about desired predictive functions into neural network training. In this work, we approach regularization in neural networks from a probabilistic perspective and show that by viewing parameter-space regularization as specifying an empirical prior distribution over the model parameters, we can derive a probabilistically well-motivated regularization technique that allows explicitly encoding information about desired predictive functions into neural network training. This method—which we refer to as function-space empirical Bayes (FS-EB)—includes both parameter- and function-space regularization, is mathematically simple, easy to implement, and incurs only minimal computational overhead compared to standard regularization techniques. We evaluate the utility of this regularization technique empirically and demonstrate that the proposed method leads to near-perfect semantic shift detection, highly-calibrated predictive uncertainty estimates, successful task adaption from pre-trained models, and improved generalization under covariate shift</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">rudner2023fseb</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{F}unction-{S}pace {R}egularization in {N}eural {N}etworks: {A} {P}robabilistic {P}erspective}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rudner, Tim G. J. and Kapoor, Sanyam and Qiu, Shikai and Wilson, Andrew Gordon}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 40th International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{Proceedings of Machine Learning Research}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{PMLR}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://timrudner.com/fseb}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/rudner2022fsvi.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="rudner2022fsvi.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="rudner2022fsvi" class="col-sm-8"> <div class="title">Tractable Function-Space Variational Inference in Bayesian Neural Networks</div> <div class="author"> <em><b>Tim G. J. Rudner</b></em>, Zonghao Chen, Yee Whye Teh, and Yarin Gal</div> <div class="periodical"> <em>Advances in Neural Information Processing Systems</em> <b>(NeurIPS)</b>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/forum?id=OQs0pLKGGpS" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/poster/rudner2022fsvi.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://github.com/timrudner/FSVI" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Reliable predictive uncertainty estimation plays an important role in enabling the deployment of neural networks to safety-critical settings. A popular approach for estimating the predictive uncertainty of neural networks is to define a prior distribution over the network parameters, infer an approximate posterior distribution, and use it to make stochastic predictions. However, explicit inference over neural network parameters makes it difficult to incorporate meaningful prior information about the data-generating process into the model. In this paper, we pursue an alternative approach. Recognizing that the primary object of interest in most settings is the distribution over functions induced by the posterior distribution over neural network parameters, we frame Bayesian inference in neural networks explicitly as inferring a posterior distribution over functions and propose a scalable function-space variational inference method that allows incorporating prior information and results in reliable predictive uncertainty estimates. We show that the proposed method leads to state-of-the-art uncertainty estimation and predictive performance on a range of prediction tasks and demonstrate that it performs well on a challenging safety-critical medical diagnosis task in which reliable uncertainty estimation is essential.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">rudner2022fsvi</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{T}ractable {F}unction-{S}pace {V}ariational {I}nference in {B}ayesian {N}eural {N}etworks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rudner, Tim G. J. and Chen, Zonghao and Teh, Yee Whye and Gal, Yarin}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems 35}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=OQs0pLKGGpS}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <br> <h4>Robustness to Distribution Shifts</h4> <h2 class="bibliography"></h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/tran2022plex.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="tran2022plex.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="tran2022plex" class="col-sm-8"> <div class="title">Plex: Towards Reliability Using Pretrained Large Model Extensions</div> <div class="author"> Dustin Tran, Jeremiah Liu, Michael W. Dusenberry, Du Phan, Mark Collier, Jie Ren, Kehang Han, Zi Wang, Zelda Mariet, Huiyi Hu, Neil Band, <em><b>Tim G. J. Rudner</b></em>, Karan Singhal, Zachary Nado, Joost Amersfoort, Andreas Kirsch, Rodolphe Jenatton, Nithum Thain, Honglin Yuan, Kelly Buchanan, Kevin Murphy, D. Sculley, Yarin Gal, Zoubin Ghahramani, Jasper Snoek, and Balaji Lakshminarayanan</div> <div class="periodical"> <em>ICML 2022 Workshop on Pre-training: Perspectives, Pitfalls, and Paths Forward</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2207.07411" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://ai.googleblog.com/2022/07/towards-reliability-in-deep-learning.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Blog</a> <a href="https://github.com/google/uncertainty-baselines/tree/main/experimental/plex" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>A recent trend in artificial intelligence is the use of pretrained models for language and vision tasks, which have achieved extraordinary performance but also puzzling failures. Probing these models’ abilities in diverse ways is therefore critical to the field. In this paper, we explore the reliability of models, where we define a reliable model as one that not only achieves strong predictive performance but also performs well consistently over many decision-making tasks involving uncertainty (e.g., selective prediction, open set recognition), robust generalization (e.g., accuracy and proper scoring rules such as log-likelihood on in- and out-of-distribution datasets), and adaptation (e.g., active learning, few-shot uncertainty). We devise 10 types of tasks over 40 datasets in order to evaluate different aspects of reliability on both vision and language domains. To improve reliability, we developed ViT-Plex and T5-Plex, pretrained large model extensions for vision and language modalities, respectively. Plex greatly improves the state-of-the-art across reliability tasks, and simplifies the traditional protocol as it improves the out-of-the-box performance and does not require designing scores or tuning the model for each task. We demonstrate scaling effects over model sizes up to 1B parameters and pretraining dataset sizes up to 4B examples. We also demonstrate Plex’s capabilities on challenging tasks including zero-shot open set recognition, active learning, and uncertainty in conversational language understanding.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tran2022plex</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tran, Dustin and Liu, Jeremiah and Dusenberry, Michael W. and Phan, Du and Collier, Mark and Ren, Jie and Han, Kehang and Wang, Zi and Mariet, Zelda and Hu, Huiyi and Band, Neil and Rudner, Tim G. J. and Singhal, Karan and Nado, Zachary and van Amersfoort, Joost and Kirsch, Andreas and Jenatton, Rodolphe and Thain, Nithum and Yuan, Honglin and Buchanan, Kelly and Murphy, Kevin and Sculley, D. and Gal, Yarin and Ghahramani, Zoubin and Snoek, Jasper and Lakshminarayanan, Balaji}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{P}lex: {T}owards {R}eliability {U}sing {P}retrained {L}arge {M}odel {E}xtensions}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICML 2022 Workshop on Pre-training: Perspectives, Pitfalls, and Paths Forward}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/band2021benchmarking.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="band2021benchmarking.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="band2021benchmarking" class="col-sm-8"> <div class="title">Benchmarking Bayesian Deep Learning on Diabetic Retinopathy Detection Tasks</div> <div class="author"> Neil Band*, <em><b>Tim G. J. Rudner*</b></em>, Qixuan Feng, Angelos Filos, Zachary Nado, Michael W. Dusenberry, Ghassen Jerfel, Dustin Tran, and Yarin Gal</div> <div class="periodical"> <em>Advances in Neural Information Processing Systems</em> <b>(NeurIPS)</b>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2211.12717" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/poster/band2021benchmarking.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://github.com/google/uncertainty-baselines/tree/main/baselines/diabetic_retinopathy_detection" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Bayesian deep learning seeks to equip deep neural networks with the ability to precisely quantify their predictive uncertainty, and has promised to make deep learning more reliable for safety-critical real-world applications. Yet, existing Bayesian deep learning methods fall short of this promise; new methods continue to be evaluated on unrealistic test beds that do not reflect the complexities of downstream real-world tasks that would benefit most from reliable uncertainty quantification. We propose the RETINA Benchmark, a set of real-world tasks that accurately reflect such complexities and are designed to assess the reliability of predictive models in safety-critical scenarios. Specifically, we curate two publicly available datasets of high-resolution human retina images exhibiting varying degrees of diabetic retinopathy, a medical condition that can lead to blindness, and use them to design a suite of automated diagnosis tasks that require reliable predictive uncertainty quantification. We use these tasks to benchmark well-established and state-of-the-art Bayesian deep learning methods on task-specific evaluation metrics. We provide an easy-to-use codebase for fast and easy benchmarking following reproducibility and software design principles. We provide implementations of all methods included in the benchmark as well as results computed over 100 TPU days, 20 GPU days, 400 hyperparameter configurations, and evaluation on at least 6 random seeds each.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">band2021benchmarking</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{B}enchmarking {B}ayesian {D}eep {L}earning {o}n {D}iabetic {R}etinopathy {D}etection {T}asks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Band*, Neil and Rudner*, Tim G. J. and Feng, Qixuan and Filos, Angelos and Nado, Zachary and Dusenberry, Michael W. and Jerfel, Ghassen and Tran, Dustin and Gal, Yarin}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems 34}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <br> <h4>Probabilistic Sequential Decision-Making</h4> <h2 class="bibliography"></h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/rudner2022sfsvi.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="rudner2022sfsvi.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="rudner2022sfsvi" class="col-sm-8"> <div class="title">Continual Learning via Sequential Function-Space Variational Inference</div> <div class="author"> <em><b>Tim G. J. Rudner</b></em>, Freddie Bickford Smith, Qixuan Feng, Yee Whye Teh, and Yarin Gal</div> <div class="periodical"> <em>International Conference on Machine Learning</em> <b>(ICML)</b>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://proceedings.mlr.press/v162/rudner22a.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/poster/rudner2022sfsvi.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://github.com/timrudner/S-FSVI" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://icml.cc/virtual/2022/spotlight/18270" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Talk</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Sequential Bayesian inference over predictive functions is a natural framework for continual learning from streams of data. However, applying it to neural networks has proved challenging in practice. Addressing the drawbacks of existing techniques, we propose an optimization objective derived by formulating continual learning as sequential function-space variational inference. In contrast to existing methods that regularize neural network parameters directly, this objective allows parameters to vary widely during training, enabling better adaptation to new tasks. Compared to objectives that directly regularize neural network predictions, the proposed objective allows for more flexible variational distributions and more effective regularization. We demonstrate that, across a range of task sequences, neural networks trained via sequential function-space variational inference achieve better predictive accuracy than networks trained with related methods while depending less on maintaining a set of representative points from previous tasks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">rudner2022sfsvi</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rudner, Tim G. J. and Smith, Freddie Bickford and Feng, Qixuan and Teh, Yee Whye and Gal, Yarin}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{C}ontinual {L}earning via {S}equential {F}unction-{S}pace {V}ariational {I}nference}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 39th International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{Proceedings of Machine Learning Research}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{PMLR}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/rudner2021odrl.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="rudner2021odrl.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="rudner2021odrl" class="col-sm-8"> <div class="title">Outcome-Driven Reinforcement Learning via Variational Inference</div> <div class="author"> <em><b>Tim G. J. Rudner*</b></em>, Vitchyr H. Pong*, Rowan McAllister, Yarin Gal, and Sergey Levine</div> <div class="periodical"> <em>Advances in Neural Information Processing Systems</em> <b>(NeurIPS)</b>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2104.10190" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/poster/rudner2021odrl.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://sites.google.com/view/od-ac" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>While reinforcement learning algorithms provide automated acquisition of optimal policies, practical application of such methods requires a number of design decisions, such as manually designing reward functions that not only define the task, but also provide sufficient shaping to accomplish it. In this paper, we view reinforcement learning as inferring policies that achieve desired outcomes, rather than as a problem of maximizing rewards. To solve this inference problem, we establish a novel variational inference formulation that allows us to derive a well-shaped reward function which can be learned directly from environment interactions. From the corresponding variational objective, we also derive a new probabilistic Bellman backup operator and use it to develop an off-policy algorithm to solve goal-directed tasks. We empirically demonstrate that this method eliminates the need to hand-craft reward functions for a suite of diverse manipulation and locomotion tasks and leads to effective goal-directed behaviors.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">rudner2021odrl</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{O}utcome-{D}riven {R}einforcement {L}earning via {V}ariational {I}nference}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rudner*, Tim G. J. and Pong*, Vitchyr H. and McAllister, Rowan and Gal, Yarin and Levine, Sergey}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems 34}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <br> <h4>Clinical Decision-Making and Drug Discovery</h4> <h2 class="bibliography"></h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/gruver2023nos.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="gruver2023nos.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gruver2023nos" class="col-sm-8"> <div class="title">Protein Design with Guided Discrete Diffusion</div> <div class="author"> Nate Gruver, Samuel Stanton, Nathan C. Frey, <em><b>Tim G. J. Rudner</b></em>, Isidro Hotzel, Julien Lafrance-Vanasse, Arvind Rajpal, Kyunghyun Cho, and Andrew Gordon Wilson</div> <div class="periodical"> <em>Advances in Neural Information Processing Systems</em> <b>(NeurIPS)</b>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/pdf?id=MfiK69Ga6p" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/ngruver/NOS" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://neurips.cc/virtual/2023/poster/71899" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Talk</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>A popular approach to protein design is to combine a generative model with a discriminative model for conditional sampling. The generative model samples plausible sequences while the discriminative model guides a search for sequences with high fitness. Given its broad success in conditional sampling, classifier-guided diffusion modeling is a promising foundation for protein design, leading many to develop guided diffusion models for structure with inverse folding to recover sequences. In this work, we propose diffusioN Optimized Sampling (NOS), a guidance method for discrete diffusion models that follows gradients in the hidden states of the denoising network. NOS makes it possible to perform design directly in sequence space, circumventing significant limitations of structure-based methods, including scarce data and challenging inverse design. Moreover, we use NOS to generalize LaMBO, a Bayesian optimization procedure for sequence design that facilitates multiple objectives and edit-based constraints. The resulting method, LaMBO-2, enables discrete diffusions and stronger performance with limited edits through a novel application of saliency maps. We apply LaMBO-2 to a real-world protein design task, optimizing antibodies for higher expression yield and binding affinity to several therapeutic targets under locality and developability constraints, attaining a 99% expression rate and 40% binding rate in exploratory in vitro experiments.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gruver2023nos</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Protein Design with Guided Discrete Diffusion}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gruver, Nate and Stanton, Samuel and Frey, Nathan C. and Rudner, Tim G. J. and Hotzel, Isidro and Lafrance-Vanasse, Julien and Rajpal, Arvind and Cho, Kyunghyun and Wilson, Andrew Gordon}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems 36}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="lechuga2023m2d2" class="col-sm-8"> <div class="title">Informative Priors Improve the Reliability of Phenotype Prediction from Multimodal Healthcare Data</div> <div class="author"> Julian Lechuga Lopez, <em><b>Tim G. J. Rudner</b></em>, and Farah Shamout</div> <div class="periodical"> <em>Machine Learning for Health Symposium Findings (Forthcoming)</em> <b>(ML4H)</b>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Machine learning-aided clinical decision support has the potential to significantly improve patient care. However, existing efforts in this domain for principled quantification of uncertainty have largely been limited to applications of ad-hoc solutions that do not consistently improve reliability. In this work, we consider stochastic neural networks and design a tailor-made multimodal data-driven (M2D2) prior distribution over network parameters. We use simple and scalable Gaussian mean-field variational inference to train a Bayesian neural network using the M2D2 prior. We train and evaluate the proposed approach using clinical time-series data in MIMIC-IV and corresponding chest X-ray images in MIMIC-CXR for phenotype classification. Our empirical results show that the proposed method produces a more reliable predictive model compared to deterministic and Bayesian neural network baselines.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lechuga2023m2d2</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Informative Priors Improve the Reliability of Phenotype Prediction from Multimodal Healthcare Data}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lopez, Julian Lechuga and Rudner, Tim G. J. and Shamout, Farah}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Machine Learning for Health Symposium Findings (Forthcoming)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/klarner2023qsavi.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="klarner2023qsavi.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="klarner2023qsavi" class="col-sm-8"> <div class="title">Drug Discovery under Covariate Shift with Domain-Informed Prior Distributions over Functions</div> <div class="author"> Leo Klarner, <em><b>Tim G. J. Rudner</b></em>, Michael Reutlinger, Torsten Schindler, Garrett M. Morris, Charlotte Deane, and Yee Whye Teh</div> <div class="periodical"> <em>International Conference on Machine Learning</em> <b>(ICML)</b>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://proceedings.mlr.press/v202/klarner23a.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/poster/klarner2023qsavi.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://github.com/leojklarner/Q-SAVI" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://icml.cc/virtual/2023/spotlight/25037" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Talk</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Accelerating the discovery of novel and more effective therapeutics is an important pharmaceutical problem in which deep learning is playing an increasingly significant role. However, real-world drug discovery tasks are often characterized by a scarcity of labeled data and significant covariate shift—a setting that poses a challenge to standard deep learning methods. In this paper, we present Q-SAVI, a probabilistic model able to address these challenges by encoding explicit prior knowledge of the data-generating process into a prior distribution over functions, presenting researchers with a transparent and probabilistically principled way to encode data-driven modeling preferences. Building on a novel, gold-standard bioactivity dataset that facilitates a meaningful comparison of models in an extrapolative regime, we explore different approaches to induce data shift and construct a challenging evaluation setup. We then demonstrate that using Q-SAVI to integrate contextualized prior knowledge of drug-like chemical space into the modeling process affords substantial gains in predictive accuracy and calibration, outperforming a broad range of state-of-the-art self-supervised pre-training and domain adaptation techniques.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">klarner2023qsavi</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{D}rug {D}iscovery {u}nder {C}ovariate {S}hift {w}ith {D}omain-{I}nformed {P}rior {D}istributions {o}ver {F}unctions}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Klarner, Leo and Rudner, Tim G. J. and Reutlinger, Michael and Schindler, Torsten and Morris, Garrett M. and Deane, Charlotte and Teh, Yee Whye}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 40th International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{Proceedings of Machine Learning Research}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{PMLR}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <br> <h2>Policy Papers</h2> <div class="publications"> <h2 class="bibliography"></h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="rudner2023uncertainty" class="col-sm-8"> <div class="title">Key Concepts in AI Safety: Reliable Uncertainty Quantification in Machine Learning</div> <div class="author"> <em><b>Tim G. J. Rudner</b></em>, and Helen Toner</div> <div class="periodical"> <em>CSET Issue Briefs (Forthcoming)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/rudner2021specification.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="rudner2021specification.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="rudner2021specification" class="col-sm-8"> <div class="title">Key Concepts in AI Safety: Specification in Machine Learning</div> <div class="author"> <em><b>Tim G. J. Rudner</b></em>, and Helen Toner</div> <div class="periodical"> <em>CSET Issue Briefs</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-specification-in-machine-learning/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/rudner2021interpretability.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="rudner2021interpretability.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="rudner2021interpretability" class="col-sm-8"> <div class="title">Key Concepts in AI Safety: Interpretability in Machine Learning</div> <div class="author"> <em><b>Tim G. J. Rudner</b></em>, and Helen Toner</div> <div class="periodical"> <em>CSET Issue Briefs</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-interpretability-in-machine-learning" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/rudner2021robustness.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="rudner2021robustness.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="rudner2021robustness" class="col-sm-8"> <div class="title">Key Concepts in AI Safety: Robustness and Adversarial Examples</div> <div class="author"> <em><b>Tim G. J. Rudner</b></em>, and Helen Toner</div> <div class="periodical"> <em>CSET Issue Briefs</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-robustness-and-adversarial-examples" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/rudner2021aisafety.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="rudner2021aisafety.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="rudner2021aisafety" class="col-sm-8"> <div class="title">Key Concepts in AI Safety: An Overview</div> <div class="author"> <em><b>Tim G. J. Rudner</b></em>, and Helen Toner</div> <div class="periodical"> <em>CSET Issue Briefs</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-an-overview" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2023 Tim G. J. Rudner. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-NHCCB4P5ZW"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-NHCCB4P5ZW");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>